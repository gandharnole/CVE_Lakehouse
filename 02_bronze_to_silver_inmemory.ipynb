{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82dbb38e-810e-4481-8a1b-b7b537c9fb9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ df_2024 rebuilt: 3083 rows\n"
     ]
    }
   ],
   "source": [
    "import requests, io, zipfile, json\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "URL = \"https://github.com/CVEProject/cvelistV5/archive/refs/heads/main.zip\"\n",
    "resp = requests.get(URL)\n",
    "z = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "import re\n",
    "json_files = [f for f in z.namelist() if re.search(r\"cves/2024/.+\\.json$\", f)]\n",
    "records = []\n",
    "for name in json_files[:5000]:  # smaller subset to stay within CE memory\n",
    "    try:\n",
    "        with z.open(name) as f:\n",
    "            j = json.load(f)\n",
    "            meta = j.get(\"cveMetadata\", {})\n",
    "            cna = j.get(\"containers\", {}).get(\"cna\", {}) if isinstance(j.get(\"containers\"), dict) else {}\n",
    "            records.append({\n",
    "                \"cveId\": meta.get(\"cveId\"),\n",
    "                \"state\": meta.get(\"state\"),\n",
    "                \"datePublished\": meta.get(\"datePublished\"),\n",
    "                \"dateUpdated\": meta.get(\"dateUpdated\"),\n",
    "                \"title\": cna.get(\"title\"),\n",
    "                \"metrics_json\": json.dumps(cna.get(\"metrics\", [])),\n",
    "                \"affected_json\": json.dumps(cna.get(\"affected\", [])),\n",
    "                \"descriptions_json\": json.dumps(cna.get(\"descriptions\", []))\n",
    "            })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "df_raw = spark.createDataFrame(records)\n",
    "df_2024 = (df_raw\n",
    "           .withColumn(\"date_published_ts\", F.to_timestamp(\"datePublished\"))\n",
    "           .filter(F.year(\"date_published_ts\") == 2024))\n",
    "\n",
    "print(\"✅ df_2024 rebuilt:\", df_2024.count(), \"rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9821a95d-5181-44df-8c7f-ffe787dcf5c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Silver Core table ready: 3083 rows\n+-------------+-----+---------+-----------------------+-----------------------+----------+-------------+\n|cveId        |title|state    |published_ts           |updated_ts             |cvss_score|cvss_severity|\n+-------------+-----+---------+-----------------------+-----------------------+----------+-------------+\n|CVE-2024-0001|NULL |PUBLISHED|2024-09-23 17:25:00.509|2024-09-23 17:57:24.819|NULL      |CRITICAL     |\n|CVE-2024-0002|NULL |PUBLISHED|2024-09-23 17:26:08.811|2024-09-23 18:04:46.783|NULL      |CRITICAL     |\n|CVE-2024-0003|NULL |PUBLISHED|2024-09-23 17:27:30.114|2024-09-24 13:28:44.669|9.1       |CRITICAL     |\n|CVE-2024-0004|NULL |PUBLISHED|2024-09-23 17:28:53.664|2024-09-24 13:37:36.931|9.1       |CRITICAL     |\n|CVE-2024-0005|NULL |PUBLISHED|2024-09-23 17:34:11.321|2024-09-24 13:49:20.771|9.1       |CRITICAL     |\n+-------------+-----+---------+-----------------------+-----------------------+----------+-------------+\nonly showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "\n",
    "# UDF to extract CVSS v3.x fields safely\n",
    "def extract_cvss_fields(metrics_json):\n",
    "    try:\n",
    "        metrics = json.loads(metrics_json)\n",
    "        if isinstance(metrics, list) and metrics:\n",
    "            m = metrics[0]\n",
    "            v31 = m.get(\"cvssV3_1\", {})\n",
    "            v30 = m.get(\"cvssV3\", {})\n",
    "            base_score = v31.get(\"baseScore\") or v30.get(\"baseScore\")\n",
    "            base_sev   = v31.get(\"baseSeverity\") or v30.get(\"baseSeverity\")\n",
    "            return (base_score, base_sev)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return (None, None)\n",
    "\n",
    "# Register UDF\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, StringType\n",
    "spark.udf.register(\"extract_cvss_fields\", extract_cvss_fields)\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"score\", DoubleType()),\n",
    "    StructField(\"severity\", StringType())\n",
    "])\n",
    "\n",
    "df_core = (\n",
    "    df_2024\n",
    "    .withColumn(\"cvss_struct\", F.udf(extract_cvss_fields, schema)(F.col(\"metrics_json\")))\n",
    "    .withColumn(\"cvss_score\", F.col(\"cvss_struct.score\"))\n",
    "    .withColumn(\"cvss_severity\", F.col(\"cvss_struct.severity\"))\n",
    "    .withColumn(\"published_ts\", F.to_timestamp(\"datePublished\"))\n",
    "    .withColumn(\"updated_ts\",   F.to_timestamp(\"dateUpdated\"))\n",
    "    .select(\"cveId\",\"title\",\"state\",\"published_ts\",\"updated_ts\",\"cvss_score\",\"cvss_severity\")\n",
    ")\n",
    "\n",
    "df_core.createOrReplaceTempView(\"cve_silver_core\")\n",
    "\n",
    "print(\"✅ Silver Core table ready:\", df_core.count(), \"rows\")\n",
    "df_core.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66c4baf9-2841-4ec5-886c-960a412e0729",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Silver Affected table ready: 4989 rows\n+-------------+------------------+-------------------+\n|cveId        |vendor            |product            |\n+-------------+------------------+-------------------+\n|CVE-2024-0001|Pure Storage      |FlashArray         |\n|CVE-2024-0002|PureStorage       |FlashArray         |\n|CVE-2024-0003|PureStorage       |FlashArray         |\n|CVE-2024-0004|PureStorage       |FlashArray         |\n|CVE-2024-0005|PureStorage       |FlashArray         |\n|CVE-2024-0005|PureStorage       |FlashBlade         |\n|CVE-2024-0006|YugabyteDB        |YugabyteDB Anywhere|\n|CVE-2024-0007|Palo Alto Networks|PAN-OS             |\n|CVE-2024-0007|Palo Alto Networks|Prisma Access      |\n|CVE-2024-0007|Palo Alto Networks|Cloud NGFW         |\n+-------------+------------------+-------------------+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import types as T\n",
    "\n",
    "# Schema for the affected array\n",
    "affected_schema = T.ArrayType(\n",
    "    T.StructType([\n",
    "        T.StructField(\"vendor\", T.StringType()),\n",
    "        T.StructField(\"product\", T.StringType()),\n",
    "        T.StructField(\"versions\", T.ArrayType(T.MapType(T.StringType(), T.StringType())))\n",
    "    ])\n",
    ")\n",
    "\n",
    "df_aff = (\n",
    "    df_2024\n",
    "    .withColumn(\"affected_arr\", F.from_json(\"affected_json\", affected_schema))\n",
    "    .withColumn(\"aff_exploded\", F.explode_outer(\"affected_arr\"))\n",
    "    .select(\n",
    "        F.col(\"cveId\"),\n",
    "        F.col(\"aff_exploded.vendor\").alias(\"vendor\"),\n",
    "        F.col(\"aff_exploded.product\").alias(\"product\")\n",
    "    )\n",
    ")\n",
    "\n",
    "df_aff.createOrReplaceTempView(\"cve_silver_affected\")\n",
    "\n",
    "print(\"✅ Silver Affected table ready:\", df_aff.count(), \"rows\")\n",
    "df_aff.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ef2561-96d5-40c3-8c85-65abaa598daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique CVEs in core: 3083\nUnique CVEs in affected: 3083\nTop vendors by number of CVEs:\n+-------------+-----+\n|vendor       |count|\n+-------------+-----+\n|Red Hat      |1260 |\n|code-projects|159  |\n|Mozilla      |144  |\n|n/a          |114  |\n|Unknown      |97   |\n|Google       |87   |\n|PHPGurukul   |78   |\n|D-Link       |73   |\n|NVIDIA       |72   |\n|NULL         |70   |\n+-------------+-----+\nonly showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "print(\"Unique CVEs in core:\", df_core.select(\"cveId\").distinct().count())\n",
    "print(\"Unique CVEs in affected:\", df_aff.select(\"cveId\").distinct().count())\n",
    "\n",
    "print(\"Top vendors by number of CVEs:\")\n",
    "df_aff.groupBy(\"vendor\").count().orderBy(F.desc(\"count\")).show(10, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09f7cce3-eb5a-4e0f-8bc3-2cc8b51fdc97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+---------+\n|vendor        |cve_count|avg_score|\n+--------------+---------+---------+\n|code-projects |158      |5.74     |\n|n/a           |114      |5.44     |\n|Unknown       |92       |9.8      |\n|Google        |87       |NULL     |\n|PHPGurukul    |78       |5.23     |\n|SourceCodester|70       |4.66     |\n|IrfanView     |70       |NULL     |\n|Red Hat       |58       |NULL     |\n|1000 Projects |50       |NULL     |\n|Mozilla       |50       |NULL     |\n+--------------+---------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df_core.createOrReplaceTempView(\"cve_core\")\n",
    "df_aff.createOrReplaceTempView(\"cve_affected\")\n",
    "\n",
    "# Example join query\n",
    "spark.sql(\"\"\"\n",
    "SELECT a.vendor, COUNT(DISTINCT a.cveId) AS cve_count,\n",
    "       ROUND(AVG(c.cvss_score),2) AS avg_score\n",
    "FROM cve_affected a\n",
    "JOIN cve_core c ON a.cveId = c.cveId\n",
    "WHERE a.vendor IS NOT NULL\n",
    "GROUP BY a.vendor\n",
    "ORDER BY cve_count DESC\n",
    "LIMIT 10\n",
    "\"\"\").show(truncate=False)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_bronze_to_silver_inmemory",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}